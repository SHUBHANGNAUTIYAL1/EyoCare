{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [21/Jul/2023 12:42:24] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [21/Jul/2023 12:42:24] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA868C1D80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 164ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [21/Jul/2023 12:42:29] \"GET /get?msg=eyocare HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('popular')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "from flask import Flask, render_template, request,send_file\n",
    "from PIL import Image\n",
    "import os\n",
    "from keras.models import load_model\n",
    "model = load_model('model.pkl')\n",
    "import json\n",
    "import random\n",
    "import tensorflow\n",
    "from tensorflow.keras import models\n",
    "import cv2\n",
    "intents = json.loads(open('data2.json').read())\n",
    "words = pickle.load(open('texts.pkl','rb'))\n",
    "classes = pickle.load(open('labels.pkl','rb'))\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "model1 = models.load_model('model1')\n",
    "model2 = models.load_model('model2')\n",
    "model3=models.load_model('model3')\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def generate_form(fields):\n",
    "    form_html = '<form method=\"POST\" action=\"/get\">'\n",
    "    for field in fields:\n",
    "        form_html += f'<label for=\"{field[\"name\"]}\">{field[\"label\"]}</label><br>'\n",
    "        if field[\"type\"] == \"radio\":\n",
    "            for option in field[\"options\"]:\n",
    "                form_html += f'<input type=\"radio\" name=\"{field[\"name\"]}\" value=\"{option[\"value\"]}\">{option[\"label\"]}<br>'\n",
    "            \n",
    "    form_html += '<input type=\"submit\" value=\"Submit\"></form>'\n",
    "    return form_html\n",
    "\n",
    "\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            if 'form' in i:\n",
    "                form = i['form']\n",
    "                form_html = generate_form(form['fields'])\n",
    "                return form_html\n",
    "            else:\n",
    "                result = random.choice(i['responses'])\n",
    "                break\n",
    "    return result\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "# def getResponse(ints, intents_json):\n",
    "#     tag = ints[0]['intent']\n",
    "#     list_of_intents = intents_json['intents']\n",
    "#     for i in list_of_intents:\n",
    "#         if(i['tag']== tag):\n",
    "#             result = random.choice(i['responses'])\n",
    "#             break\n",
    "#     return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "final_prediction = []\n",
    "\n",
    "def disease_prediction(image):\n",
    "  final_prediction.clear()  \n",
    "  img_bytes = image.read()  # Read the image bytes\n",
    "  nparr = np.frombuffer(img_bytes, np.uint8)  # Convert bytes to NumPy array\n",
    "  img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)  \n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)) \n",
    "  predictions_model1 = model1.predict(np.array([img]))\n",
    "  predicted_class_model1 = np.argmax(predictions_model1)\n",
    "  \n",
    "  predictions_model2 = model2.predict(np.array([img]))\n",
    "  predicted_class_model2 = np.argmax(predictions_model2)\n",
    "  \n",
    "  predictions_model3 = model3.predict(np.array([img]))\n",
    "  predicted_class_model3 = np.argmax(predictions_model3)\n",
    "  \n",
    "  if predicted_class_model1 == 0:\n",
    "    final_prediction.append(\"cataract\")\n",
    "  elif predicted_class_model1 == 1:\n",
    "    final_prediction.append(\"normal\")\n",
    "\n",
    "  if predicted_class_model2 == 0:\n",
    "    final_prediction.append(\"Jaundice\")\n",
    "  elif predicted_class_model2 == 1:\n",
    "    final_prediction.append(\"normal\")\n",
    "    \n",
    "  if predicted_class_model3 == 0:\n",
    "    final_prediction.append(\"Strabismus\")\n",
    "  elif predicted_class_model3 == 1:\n",
    "    final_prediction.append(\"normal\")  \n",
    "    \n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "     userText = request.args.get('msg') \n",
    "     if userText==\"cataractCheck\" or userText==\"JaundiceCheck\" or userText==\"StrabismusCheck\" or userText==\"full\":\n",
    "         return final_prediction\n",
    "     return chatbot_response(userText)\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/upload\",methods=['POST'])\n",
    "def get_bot_response2():\n",
    "    if 'image' in request.files :\n",
    "      image=request.files['image']\n",
    "      \n",
    "      #img=image.save(os.path.join(os.getcwd(), 'image.jpg'))\n",
    "      disease_prediction(image)\n",
    "      return \"mk\"\n",
    "    else :\n",
    "        return \"lk\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
